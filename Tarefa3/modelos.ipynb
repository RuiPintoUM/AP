{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizem Profunda\n",
    "### Tarefa III\n",
    "1. Rúben Gonçalo Araújo da Silva pg57900   \n",
    "2. José Luis Fraga Costa pg55970\n",
    "3. Pedro Miguel Costa Azevedo pg57897\n",
    "4. Rui Pedro Fernandes Madeira Pinto pg56010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports\n",
    "\n",
    "1. pandas\n",
    "2. tensorflow\n",
    "3. sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/combined_dataset_treino.csv\")\n",
    "test_ids = df[\"ID\"].fillna(\"\")\n",
    "test_texts = df[\"Text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função pra gravar em csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(results_df,name):\n",
    "    os.makedirs(\"submissao2\", exist_ok=True)\n",
    "    path = f\"submissao2/{name}.csv\"\n",
    "    # Extract numeric part of ID and sort\n",
    "\n",
    "    #aplica regex D{0-9}\n",
    "    results_df[\"numeric_id\"] = results_df[\"ID\"].str.replace(r\"D\\d+-\", \"\", regex=True).astype(int)\n",
    "    results_df = results_df.sort_values(\"numeric_id\").drop(columns=[\"numeric_id\"])  # Sort and drop temp column\n",
    "    results_df[\"combined\"] = results_df[\"ID\"] + \" \" + results_df[\"Label\"]\n",
    "    results_df[[\"combined\"]].to_csv(path, index=False, header=False, quoting=csv.QUOTE_MINIMAL)\n",
    "    print(f\"Predictions saved to '{path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"Text\"])  \n",
    "sequences = tokenizer.texts_to_sequences(df[\"Text\"])\n",
    "padded_sequences = pad_sequences(sequences, padding=\"post\")\n",
    "df[\"Label\"] = df[\"Label\"].map({\"Human\": 0, \"AI\": 1}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split (divisão de dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test, id_temp, test_ids = train_test_split(\n",
    "    padded_sequences, df[\"Label\"], df[\"ID\"], test_size=0.15, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val, id_train, id_val = train_test_split(\n",
    "    X_temp, y_temp, id_temp, test_size=0.1765, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "(atualmente só no DNN e RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     \n",
    "    patience=10,            \n",
    "    restore_best_weights=True,  \n",
    "    mode='min'               \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\Desktop\\Minho\\MEI\\SI\\AP\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.7690 - loss: 0.4252 - val_accuracy: 0.9934 - val_loss: 0.0120\n",
      "Epoch 2/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - accuracy: 0.9997 - loss: 0.0029 - val_accuracy: 0.9951 - val_loss: 0.0088\n",
      "Epoch 3/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9951 - val_loss: 0.0114\n",
      "Epoch 4/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 4.5427e-04 - val_accuracy: 0.9934 - val_loss: 0.0237\n",
      "Epoch 5/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.2087e-04 - val_accuracy: 0.9951 - val_loss: 0.0131\n",
      "Epoch 6/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.8746e-04 - val_accuracy: 0.9951 - val_loss: 0.0112\n",
      "Epoch 7/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.0671e-04 - val_accuracy: 0.9967 - val_loss: 0.0137\n",
      "Epoch 8/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 6.3180e-05 - val_accuracy: 0.9951 - val_loss: 0.0163\n",
      "Epoch 9/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.4727e-04 - val_accuracy: 0.9967 - val_loss: 0.0167\n",
      "Epoch 10/10\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 9.9839e-05 - val_accuracy: 0.9967 - val_loss: 0.0162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23a81fc2810>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_dnn = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=X_train.shape[1]),\n",
    "    Flatten(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_dnn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_dnn.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=X_train.shape[1]),\n",
    "    SimpleRNN(64, return_sequences=True),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_rnn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_rnn.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=X_train.shape[1]),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_lstm.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=X_train.shape[1]),\n",
    "    GRU(64, return_sequences=True),\n",
    "    GRU(32),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "\n",
    "model_gru.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_gru.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert (nao funciona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_bert = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Pré-processar o texto\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"tf\")\n",
    "\n",
    "train_encodings = encode_texts(df[\"Text\"].tolist())\n",
    "train_labels = df[\"Label\"].values\n",
    "\n",
    "# Treinar o modelo\n",
    "model_bert.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_bert.fit(train_encodings[\"input_ids\"], train_labels, epochs=3, batch_size=8, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correr Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Predictions saved to 'submissao2/submission_dnn.csv'\n"
     ]
    }
   ],
   "source": [
    "y_pred_dnn = (model_dnn.predict(X_test) > 0.5).astype(\"int32\")\n",
    "labels_dnn = [\"AI\" if pred == 1 else \"Human\" for pred in y_pred_dnn.flatten()]\n",
    "results_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"Label\": labels_dnn\n",
    "})\n",
    "# Combine ID and Label with a space\n",
    "to_csv(results_df,\"submission_dnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rnn = (model_rnn.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred_rnn))\n",
    "\n",
    "labels_rnn = [\"IA\" if pred > 0.5 else \"Human\" for pred in y_pred_rnn.flatten()]\n",
    "results_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"Label\": labels_rnn\n",
    "})\n",
    "\n",
    "to_csv(results_df,\"submission_rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm = (model_lstm.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred_lstm))\n",
    "\n",
    "labels_lstm = [\"IA\" if pred > 0.5 else \"Human\" for pred in y_pred_lstm.flatten()]\n",
    "results_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"Label\": labels_lstm\n",
    "})\n",
    "\n",
    "# Salvar como CSV\n",
    "to_csv(results_df,\"submission_lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gru = (model_gru.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred_gru))\n",
    "\n",
    "labels_gru = [\"IA\" if pred > 0.5 else \"Human\" for pred in y_pred_gru.flatten()]\n",
    "results_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"Label\": labels_gru\n",
    "})\n",
    "\n",
    "# Salvar como CSV\n",
    "to_csv(results_df,\"submission_gru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bert = model_bert.predict(train_encodings[\"input_ids\"]).logits.numpy().argmax(axis=1)\n",
    "print(classification_report(train_labels, y_pred_bert))\n",
    "\n",
    "labels_bert = [\"IA\" if pred > 0.5 else \"Human\" for pred in y_pred_bert.flatten()]\n",
    "results_df = pd.DataFrame({\n",
    "    \"ID\": test_ids,\n",
    "    \"Label\": labels_bert\n",
    "})\n",
    "\n",
    "# Salvar como CSV\n",
    "to_csv(results_df,\"submission_bert\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
