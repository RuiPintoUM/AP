{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizem Profunda - GRU\n",
    "### Tarefa III\n",
    "1. Rúben Gonçalo Araújo da Silva pg57900   \n",
    "2. José Luis Fraga Costa pg55970\n",
    "3. Pedro Miguel Costa Azevedo pg57897\n",
    "4. Rui Pedro Fernandes Madeira Pinto pg56010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports\n",
    "\n",
    "1. pandas\n",
    "2. tensorflow\n",
    "3. sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, Flatten, Dense, Dropout, SimpleRNN, GRU, LSTM, \n",
    "    Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, \n",
    "    BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    TFAutoModelForSequenceClassification, \n",
    "    pipeline\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/small_dataset.csv\")\n",
    "df = df.dropna(subset=[\"Label\"])\n",
    "df[\"Text\"] = df[\"Text\"].fillna(\"\")\n",
    "all_ids = df[\"ID\"].fillna(\"\")\n",
    "test_ids = df[\"ID\"].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função pra gravar em csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(results_df, name):\n",
    "    os.makedirs(\"submissao2\", exist_ok=True)\n",
    "    path = f\"submissao2/{name}.csv\"\n",
    "    filtered_df = results_df[results_df[\"ID\"].notna() & (results_df[\"ID\"] != \"\")].copy()\n",
    "    filtered_df['prefix'] = filtered_df['ID'].str.extract(r'(D[0-9])')\n",
    "    filtered_df['number'] = filtered_df['ID'].str.extract(r'-(\\d+)').astype(int)\n",
    "    filtered_df = filtered_df.sort_values(by=['prefix', 'number'])\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(\"ID\\t\\tLabel\\n\") \n",
    "        for index, row in filtered_df.iterrows():\n",
    "            f.write(f\"{row['ID']}\\t{row['Label']}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(model, X_data, ids, model_name):\n",
    "    # Generate predictions\n",
    "    y_pred = (model.predict(X_data) > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Convert numeric predictions to labels\n",
    "    labels = [\"Human\" if label == 0 else \"AI\" for label in y_pred]\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        \"ID\": ids,\n",
    "        \"Label\": labels\n",
    "    })\n",
    "    to_csv(results_df, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split (divisão de dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 4053, Unique texts: 4051\n",
      "Overlap between train and test after deduplication: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates by keeping the first occurrence\n",
    "df_unique = df.drop_duplicates(subset=[\"Text\"], keep=\"first\")\n",
    "\n",
    "# Check how many rows remain\n",
    "print(f\"Original dataset size: {len(df)}, Unique texts: {len(df_unique)}\")\n",
    "\n",
    "# Split the deduplicated dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df_unique[\"Text\"], df_unique[\"Label\"], test_size=0.2, random_state=42, stratify=df_unique[\"Label\"]\n",
    ")\n",
    "test_indices = test_texts.index\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "# Verify no overlap\n",
    "print(f\"Overlap between train and test after deduplication: {len(set(train_texts) & set(test_texts))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=100)\n",
    "X_val = pad_sequences(X_val_seq, maxlen=100)\n",
    "X_test = pad_sequences(X_test_seq, maxlen=100)\n",
    "\n",
    "# Convert string labels to numeric values\n",
    "label_map = {\"Human\": 0, \"AI\": 1}  # Define mapping\n",
    "y_train = np.array([label_map[label] for label in train_labels], dtype=np.float32)\n",
    "y_val = np.array([label_map[label] for label in val_labels], dtype=np.float32)\n",
    "y_test = np.array([label_map[label] for label in test_labels], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset de Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_professor = pd.read_csv(\"data/dataset3_inputs.csv\", sep=';', on_bad_lines='skip')\n",
    "data_set_professor[\"Text\"] = data_set_professor[\"Text\"].fillna(\"\")\n",
    "X_professor_seq = tokenizer.texts_to_sequences(data_set_professor[\"Text\"])\n",
    "X_professor = pad_sequences(X_professor_seq, maxlen=100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',     \n",
    "    patience=10,            \n",
    "    restore_best_weights=True,           \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5, \n",
    "    patience=3,\n",
    "    min_lr=0.00005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 49ms/step - accuracy: 0.5900 - loss: 0.8879 - val_accuracy: 0.8858 - val_loss: 0.7284 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9033 - loss: 0.4274 - val_accuracy: 0.8935 - val_loss: 0.4383 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9504 - loss: 0.2865 - val_accuracy: 0.9460 - val_loss: 0.2681 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9632 - loss: 0.2294 - val_accuracy: 0.9614 - val_loss: 0.2013 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 39ms/step - accuracy: 0.9621 - loss: 0.1831 - val_accuracy: 0.9522 - val_loss: 0.2074 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.9701 - loss: 0.1683 - val_accuracy: 0.9568 - val_loss: 0.2011 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.9831 - loss: 0.1186 - val_accuracy: 0.9630 - val_loss: 0.1813 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9858 - loss: 0.1100 - val_accuracy: 0.9645 - val_loss: 0.1623 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.9971 - loss: 0.0776 - val_accuracy: 0.9660 - val_loss: 0.1679 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9979 - loss: 0.0704 - val_accuracy: 0.9753 - val_loss: 0.1395 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.9931 - loss: 0.0763 - val_accuracy: 0.9722 - val_loss: 0.1344 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.9929 - loss: 0.0863 - val_accuracy: 0.9707 - val_loss: 0.1549 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9958 - loss: 0.0716 - val_accuracy: 0.9691 - val_loss: 0.1522 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9981 - loss: 0.0643 - val_accuracy: 0.9660 - val_loss: 0.1754 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9968 - loss: 0.0708 - val_accuracy: 0.9676 - val_loss: 0.1533 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9970 - loss: 0.0589 - val_accuracy: 0.9722 - val_loss: 0.1412 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9975 - loss: 0.0567 - val_accuracy: 0.9738 - val_loss: 0.1276 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9977 - loss: 0.0611 - val_accuracy: 0.9753 - val_loss: 0.1324 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 37ms/step - accuracy: 0.9986 - loss: 0.0566 - val_accuracy: 0.9722 - val_loss: 0.1373 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - accuracy: 0.9981 - loss: 0.0573 - val_accuracy: 0.9738 - val_loss: 0.1301 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9988 - loss: 0.0546 - val_accuracy: 0.9722 - val_loss: 0.1347 - learning_rate: 2.5000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9988 - loss: 0.0517 - val_accuracy: 0.9722 - val_loss: 0.1381 - learning_rate: 2.5000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9971 - loss: 0.0537 - val_accuracy: 0.9738 - val_loss: 0.1389 - learning_rate: 2.5000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9991 - loss: 0.0566 - val_accuracy: 0.9738 - val_loss: 0.1392 - learning_rate: 1.2500e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - accuracy: 0.9980 - loss: 0.0510 - val_accuracy: 0.9753 - val_loss: 0.1298 - learning_rate: 1.2500e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.9982 - loss: 0.0541 - val_accuracy: 0.9753 - val_loss: 0.1225 - learning_rate: 1.2500e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.9981 - loss: 0.0564 - val_accuracy: 0.9769 - val_loss: 0.1252 - learning_rate: 1.2500e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.9996 - loss: 0.0529 - val_accuracy: 0.9753 - val_loss: 0.1334 - learning_rate: 1.2500e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0496 - val_accuracy: 0.9753 - val_loss: 0.1339 - learning_rate: 1.2500e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.9981 - loss: 0.0507 - val_accuracy: 0.9738 - val_loss: 0.1337 - learning_rate: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2ae93f83bf0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=100),\n",
    "    GRU(32, dropout=0.3, recurrent_dropout=0.3),\n",
    "    BatchNormalization(),\n",
    "    Dense(16, activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(0.01))\n",
    "])\n",
    "\n",
    "# Compile\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model_gru.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train\n",
    "model_gru.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correr Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Predictions saved for dnn\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Predictions saved for simple_rnn\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Predictions saved for rnn\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Predictions saved for lstm\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Predictions saved for gru\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      1.00      0.68       420\n",
      "         1.0       0.00      0.00      0.00       391\n",
      "\n",
      "    accuracy                           0.52       811\n",
      "   macro avg       0.26      0.50      0.34       811\n",
      "weighted avg       0.27      0.52      0.35       811\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruben\\Desktop\\Minho\\MEI\\SI\\AP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ruben\\Desktop\\Minho\\MEI\\SI\\AP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ruben\\Desktop\\Minho\\MEI\\SI\\AP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "test_ids = df.loc[test_indices, \"ID\"]  # Get IDs for test split only\n",
    "predict_and_save(model_gru, X_test, test_ids, \"gru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Avaliação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "Predictions saved for dnn_professor\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
      "Predictions saved for simple_rnn_professor\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Predictions saved for rnn_professor\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Predictions saved for lstm_professor\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Predictions saved for gru_professor\n"
     ]
    }
   ],
   "source": [
    "predict_and_save(model_gru, X_professor, data_set_professor[\"ID\"], \"gru_professor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98       420\n",
      "         1.0       1.00      0.96      0.98       391\n",
      "\n",
      "    accuracy                           0.98       811\n",
      "   macro avg       0.98      0.98      0.98       811\n",
      "weighted avg       0.98      0.98      0.98       811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model_gru.predict(X_test) > 0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
